{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUFV3NGMDTNA",
        "outputId": "d01aba44-71b7-4b25-c8a7-a226da6e6e45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/383.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m378.9/383.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/231.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "optuna.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "O0CEOBfUDle9",
        "outputId": "5e943e22-e850-44c4-ac07-f8fd8a56b251"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.2.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data\n",
        "df = pd.read_excel(\"/content/updated_current.xlsx\")\n",
        "# df = pd.read_excel(\"/content/bmi_output(1).xlsx\")\n",
        "\n",
        "# ------------------ Dataset Overview ------------------\n",
        "print(\"\\n--- Dataset Overview ---\")\n",
        "print(\"\\nHead of the Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(f\"\\nTotal Rows: {df.shape[0]}\")\n",
        "print(f\"Total Columns: {df.shape[1]}\")\n",
        "\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nUnique Values Per Column:\")\n",
        "print(df.nunique())\n",
        "\n",
        "print(\"\\nDescriptive Statistics (Numeric):\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nDescriptive Statistics (Categorical):\")\n",
        "print(df.describe(include='object'))\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Drop unnecessary columns\n",
        "print(type(df))  # should be <class 'pandas.core.frame.DataFrame'>\n",
        "columns_to_drop = ['Fasting Blood Sugar', 'Postprandial Blood Sugar', 'Average Blood sugar in 3 months', 'HbA1c Levels']\n",
        "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')  # this is correct\n",
        "print(f\"Total Columns: {df.shape[1]}\")\n",
        "\n",
        "# Set default \"None\" for missing values in 'Use of medication'\n",
        "if 'Use of medication' in df.columns:\n",
        "    df['Use of medication'] = df['Use of medication'].fillna('None')\n",
        "\n",
        "# Convert Age at Diagnosis to numeric\n",
        "def age_to_numeric(x):\n",
        "    mapping = {'Under 18': 9, '18-34': 26, '35-44': 39.5, '45-54': 49.5, '55 or older': 60}\n",
        "    return mapping.get(x, np.nan)\n",
        "\n",
        "df['Age_at_Diagnosis_numeric'] = df['Age at Diagnosis'].apply(age_to_numeric)\n",
        "df = df.dropna(subset=['Age_at_Diagnosis_numeric'])\n",
        "print(df['Age_at_Diagnosis_numeric'].head())\n",
        "\n",
        "# Define updated mapping for Age Group\n",
        "age_group_mapping = {\n",
        "    'Under 18': 9,\n",
        "    '18-24': 21,\n",
        "    '25-34': 29.5,\n",
        "    '35-44': 39.5,\n",
        "    '45-54': 49.5,\n",
        "    '55-64': 59.5,\n",
        "    '65 or older': 70\n",
        "}\n",
        "\n",
        "# Apply mapping to Age Group\n",
        "df['Age_Group_numeric'] = df['Age Group'].map(age_group_mapping)\n",
        "\n",
        "# Drop rows with unmapped values (just in case)\n",
        "df = df.dropna(subset=['Age_Group_numeric'])\n",
        "\n",
        "# Check result\n",
        "print(df['Age_Group_numeric'].head())\n",
        "columns_to_drop = ['Age Group', 'Age at Diagnosis']\n",
        "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "print(df.head())\n",
        "\n",
        "#segregating input and output\n",
        "X = df.drop(columns=['Age_at_Diagnosis_numeric'])\n",
        "y = df['Age_at_Diagnosis_numeric']\n",
        "\n",
        "# Visualize target variable distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df['Age_at_Diagnosis_numeric'], bins=10, kde=True)\n",
        "plt.title('Distribution of Age at Diagnosis (Numeric)')\n",
        "plt.xlabel('Age at Diagnosis')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation Heatmap\n",
        "numeric_cols = df.select_dtypes(include=[np.number])\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(numeric_cols.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "plt.title('Correlation Heatmap of Numeric Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Boxplot for categorical features vs target\n",
        "categorical_columns = df.select_dtypes(include='object').columns\n",
        "for col in categorical_columns:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.boxplot(x=df[col], y=df['Age_at_Diagnosis_numeric'])\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(f'{col} vs Age at Diagnosis')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Feature Engineering\n",
        "def create_features(data, target):\n",
        "    data = data.copy()\n",
        "    # Access 'Weight' and 'Height' from the original df using the index of data\n",
        "    data['Weight'] = df.loc[data.index, 'Weight']\n",
        "    data['Height'] = df.loc[data.index, 'Height']\n",
        "\n",
        "    # BMI Calculation\n",
        "    data['BMI'] = data['Weight'] / ((data['Height'] / 100) ** 2)\n",
        "\n",
        "    high_bmi_rows = data[data['BMI'] >= 50]\n",
        "    if not high_bmi_rows.empty:\n",
        "        print(\"\\n Rows with BMI ‚â• 50:\")\n",
        "        print(high_bmi_rows[['Weight', 'Height', 'BMI']])\n",
        "    else:\n",
        "        print(\"There aren't any rows having BMI above 50\")\n",
        "\n",
        "    # Filter rows based on BMI range and update the target accordingly\n",
        "    valid_indices = data[(data['BMI'] >= 14) & (data['BMI'] < 50)].index\n",
        "    data = data.loc[valid_indices]\n",
        "    target = target.loc[valid_indices]  # Update the target\n",
        "\n",
        "    # Drop Height and Weight columns\n",
        "    data.drop(columns=['Height', 'Weight'], inplace=True, errors='ignore')\n",
        "\n",
        "    high_bmi_rows = data[data['BMI'] >= 50]\n",
        "    if not high_bmi_rows.empty:\n",
        "        print(\"\\n Rows with BMI ‚â• 50:\")\n",
        "        print(high_bmi_rows[['Weight', 'Height', 'BMI']])\n",
        "    else:\n",
        "        print(\"There aren't any rows having BMI above 50\")\n",
        "\n",
        "\n",
        "    return data, target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, y_train = create_features(X_train, y_train) # Pass y_train to create_features\n",
        "print(X_train.head())\n",
        "X_test, y_test = create_features(X_test, y_test) # Pass y_test to create_features\n",
        "\n",
        "print(f\"\\nTotal Rows: {df.shape[0]}\")\n",
        "\n",
        "# Column Types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Categorical Features: {categorical_features}\")\n",
        "manual_features = ['BMI','Age_Group_numeric']\n",
        "print(f\"Manual Features: {manual_features}\")\n",
        "numeric_features = [f for f in numeric_features if f not in manual_features]\n",
        "print(f\"Numeric Features: {numeric_features}\")\n",
        "\n",
        "# Preprocessing Pipelines\n",
        "numeric_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "manual_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features),\n",
        "    ('man', manual_transformer, manual_features)\n",
        "])\n",
        "\n",
        "\n",
        "# Optimize Decision Tree with Optuna\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
        "    }\n",
        "    model = DecisionTreeRegressor(random_state=42, **params)\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    preds = pipe.predict(X_test)\n",
        "    return r2_score(y_test, preds)\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "best_dt = DecisionTreeRegressor(random_state=42, **study.best_params)\n",
        "\n",
        "# Columns after create_features (still readable, pre-encoding)\n",
        "original_cols = X_train.columns.tolist()\n",
        "\n",
        "# Columns after full pipeline (numeric + one-hot + manual features)\n",
        "encoded_feature_names = list(\n",
        "    preprocessor.transformers_[1][1].named_steps[\"onehot\"]\n",
        "    .get_feature_names_out(categorical_features)\n",
        ")\n",
        "final_cols = numeric_features + encoded_feature_names + manual_features\n",
        "\n",
        "# Find removed columns\n",
        "removed_cols = [col for col in original_cols if col not in final_cols]\n",
        "\n",
        "# Print vertically\n",
        "print(\"‚ùå Removed Columns After Full Pipeline:\")\n",
        "for col in removed_cols:\n",
        "    print(\" -\", col)\n",
        "\n",
        "print(\"\\n‚úÖ Final Columns After Full Pipeline:\")\n",
        "for col in final_cols:\n",
        "    print(\" -\", col)\n",
        "\n",
        "\n",
        "# Fit the preprocessor on training data and transform\n",
        "X_train_final = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# For test data\n",
        "X_test_final = preprocessor.transform(X_test)\n",
        "\n",
        "# If it's a NumPy array, convert to DataFrame for easier viewing\n",
        "feature_names = (\n",
        "    numeric_features +\n",
        "    list(preprocessor.transformers_[1][1].named_steps[\"onehot\"].get_feature_names_out(categorical_features)) +\n",
        "    manual_features\n",
        "\n",
        ")\n",
        "X_train_final_df = pd.DataFrame(X_train_final.toarray() if hasattr(X_train_final, 'toarray') else X_train_final,\n",
        "                                columns=feature_names)\n",
        "\n",
        "print(\"üîç Final Feature Set After Full Pipeline:\")\n",
        "print(X_train_final_df.head())\n",
        "\n",
        "\n",
        "# Define all models\n",
        "all_models = {\n",
        "    \"rf\": RandomForestRegressor(n_estimators=200, random_state=42),\n",
        "    \"xgb\": XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42),\n",
        "    \"svr\": SVR(kernel='rbf', C=10, epsilon=0.1),\n",
        "    \"dt\": best_dt,\n",
        "    \"lr\": LinearRegression()\n",
        "}\n",
        "\n",
        "# Function to evaluate models\n",
        "model_scores = []\n",
        "def evaluate_model(name, y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    model_scores.append({\"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
        "    print(f\"\\n{name} Model:\")\n",
        "    print(f\"MAE: {mae:.2f}\")\n",
        "    print(f\"RMSE: {rmse:.2f}\")\n",
        "    print(f\"R¬≤: {r2:.2f}\")\n",
        "\n",
        "    # Actual vs Predicted\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.scatterplot(x=y_true, y=y_pred)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], '--', color='red')\n",
        "    plt.xlabel(\"Actual\")\n",
        "    plt.ylabel(\"Predicted\")\n",
        "    plt.title(f\"Actual vs Predicted - {name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Residual Plot\n",
        "    residuals = y_true - y_pred\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.histplot(residuals, kde=True, bins=20)\n",
        "    plt.title(f\"Residual Distribution - {name}\")\n",
        "    plt.xlabel(\"Residual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate individual base models\n",
        "for name, model in all_models.items():\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    preds = pipe.predict(X_test)\n",
        "    evaluate_model(name.upper(), y_test, preds)\n",
        "\n",
        "    # Feature importance for XGB\n",
        "xgb_model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', all_models['xgb'])\n",
        "])\n",
        "xgb_model.fit(X_train, y_train)\n",
        "feature_names = xgb_model.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
        "all_features = numeric_features + list(feature_names) + manual_features\n",
        "importances = xgb_model.named_steps['regressor'].feature_importances_\n",
        "importance_df = pd.DataFrame({'Feature': all_features, 'Importance': importances})\n",
        "importance_df['Feature'] = importance_df['Feature'].str.replace('cat__', '', regex=False)\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Feature Importance with hue\n",
        "top_n = 20\n",
        "top_features = importance_df.head(top_n).copy()\n",
        "top_features['Importance Level'] = pd.qcut(top_features['Importance'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "\n",
        "cleaned_feature_names = all_features\n",
        "\n",
        "feat_imp = pd.Series(importances, index=cleaned_feature_names).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=feat_imp.values[:20], y=feat_imp.index[:20], palette=\"viridis\")\n",
        "plt.title(f'Top {top_n} Feature Importances (XGBoost)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.legend(title='Importance Level')\n",
        "plt.show()\n",
        "\n",
        "# Define stacking stages (idhr stcaking stages separately define hue hain)\n",
        "stacking_stages = [\n",
        "    [\"rf\", \"xgb\"],\n",
        "    [\"rf\", \"xgb\", \"svr\"],\n",
        "    [\"rf\", \"xgb\", \"svr\", \"dt\"],\n",
        "    [\"rf\", \"xgb\", \"svr\", \"dt\", \"lr\"]\n",
        "]\n",
        "\n",
        "# Function to get OOF predictions (this was done to overcome ipsi ka 1 R^2)\n",
        "def get_oof_predictions(model, X, y, X_test, n_splits=5):\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    oof_train = np.zeros(len(X))\n",
        "    oof_test = np.zeros(len(X_test))\n",
        "    oof_test_folds = np.empty((n_splits, len(X_test)))\n",
        "\n",
        "    for i, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('regressor', model)\n",
        "        ])\n",
        "        pipeline.fit(X_tr, y_tr)\n",
        "        oof_train[val_idx] = pipeline.predict(X_val)\n",
        "        oof_test_folds[i, :] = pipeline.predict(X_test)\n",
        "\n",
        "    oof_test[:] = oof_test_folds.mean(axis=0)\n",
        "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
        "\n",
        "    # Run stacking for each stage (idhr separately each tsacking combo ko run kiya hai)\n",
        "for stage_num, model_keys in enumerate(stacking_stages, start=1):\n",
        "    print(f\"\\nüîÅ Stacking Stage {stage_num}: {model_keys}\")\n",
        "\n",
        "    stack_train_parts, stack_test_parts = [], []\n",
        "\n",
        "    for key in model_keys:\n",
        "        model = all_models[key]\n",
        "        oof_train, oof_test = get_oof_predictions(model, X_train, y_train, X_test)\n",
        "        stack_train_parts.append(oof_train)\n",
        "        stack_test_parts.append(oof_test)\n",
        "\n",
        "    stack_train = np.hstack(stack_train_parts)\n",
        "    stack_test = np.hstack(stack_test_parts)\n",
        "\n",
        "    meta_model = Ridge(alpha=1.0)\n",
        "    meta_model.fit(stack_train, y_train)\n",
        "    final_pred = meta_model.predict(stack_test)\n",
        "\n",
        "    evaluate_model(f\"Stacking Stage {stage_num}\", y_test, final_pred)\n",
        "\n",
        "    # Plot model comparison with hue\n",
        "scores_df = pd.DataFrame(model_scores)\n",
        "scores_melted = scores_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=scores_melted, x='Model', y='Score', hue='Metric', palette='CMRmap')  #Dark2, CMRmap, Set1\n",
        "plt.title('Model Comparison by Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Model')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.legend(title='Metric')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "2Z6tap14DmsY"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}